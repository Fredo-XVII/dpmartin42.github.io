<!DOCTYPE html>
<html lang="en-us">

<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    Home | Wicked Good Data
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/lanyon.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-0d sidebar-overlay">

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  <div class="sidebar-item">
    
    <center>
    <img style="max-height: 200px; height: 80%; width: 80%; border-radius: 50%" src="/extras/DanielPMartin.jpeg" align="middle"></a></img>
    </center>
    
    <p>Quantitative Psychology PhD and Data Scientist at McKinsey & Company.</p>
    
    <p>
    
    <div style = "font-size: 30px">
    
    <script>
      emailE = ('dpmartin42@' + 'gmail.com')
	  document.write('<a href="mailto:' + emailE + '"><i class="fa fa-envelope-square"></i></a>')
	</script>
    
    <a href="https://github.com/dpmartin42"><i class="fa fa-github"></i></a>
    
    <a href="http://stackexchange.com/users/2457894/dmartin?tab=accounts"><i class="fa fa-stack-exchange"></i></a>
    
    <a href="http://linkedin.com/in/dpmartin42/"><i class="fa fa-linkedin"></i></a>
    
    </div>
    
  </div>
  

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>

    

    
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/about.html">About Me</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/blog_roll.html">Blog Roll</a>
        
      
    
      
    
      
        
      
    
      
        
          <a class="sidebar-nav-item" href="/posts.html">Posts</a>
        
      
    
      
        
          <a class="sidebar-nav-item" href="/visualizations.html">Visualizations</a>
        
      
    

  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2017. All rights reserved.
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/" title="Home">Wicked Good Data</a>
            <small>Website and blog of Daniel P. Martin</small>
          </h3>
        </div>
      </div>

      <div class="container content">
        <section class="content">
 
  
  

  <h2>
    <a href="/blogposts/r/imbalanced-classes-part-2">Handling Class Imbalance with R and Caret - Caveats when using the AUC</a>
  </h2>

  <section class="post-date">
    January 03, 2017
  </section>
  
  <p>
  
	<p>In my <a href="http://dpmartin42.github.io/blogposts/r/imbalanced-classes-part-1">last post</a>, I went over how weighting and sampling methods can help to improve predictive performance in the case of imbalanced classes. I also included an applied example with a simulated dataset that used the area under the ROC curve (AUC) as the evaluation metric. In this post, I will go over some issues to keep in mind when using the AUC in the case of imbalanced classes and highlight another metric that is useful to examine: area under the precision-recall curve (AUPRC).</p>

<p>To quickly catch up, essential code from the previous post is below:</p>

<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="kn">library</span><span class="p">(</span>dplyr<span class="p">)</span> <span class="c1"># for data manipulation</span>
<span class="kn">library</span><span class="p">(</span>caret<span class="p">)</span> <span class="c1"># for model-building</span>
<span class="kn">library</span><span class="p">(</span>DMwR<span class="p">)</span> <span class="c1"># for smote implementation</span>
<span class="kn">library</span><span class="p">(</span>purrr<span class="p">)</span> <span class="c1"># for functional programming (map)</span>
<span class="kn">library</span><span class="p">(</span>pROC<span class="p">)</span> <span class="c1"># for AUC calculations</span>
<span class="kn">library</span><span class="p">(</span>PRROC<span class="p">)</span> <span class="c1"># for Precision-Recall curve calculations</span>

<span class="kp">set.seed</span><span class="p">(</span><span class="m">2969</span><span class="p">)</span>

imbal_train <span class="o">&lt;-</span> twoClassSim<span class="p">(</span><span class="m">5000</span><span class="p">,</span>
                           intercept <span class="o">=</span> <span class="m">-25</span><span class="p">,</span>
                           linearVars <span class="o">=</span> <span class="m">20</span><span class="p">,</span>
                           noiseVars <span class="o">=</span> <span class="m">10</span><span class="p">)</span>

imbal_test  <span class="o">&lt;-</span> twoClassSim<span class="p">(</span><span class="m">5000</span><span class="p">,</span>
                           intercept <span class="o">=</span> <span class="m">-25</span><span class="p">,</span>
                           linearVars <span class="o">=</span> <span class="m">20</span><span class="p">,</span>
                           noiseVars <span class="o">=</span> <span class="m">10</span><span class="p">)</span>
  
<span class="c1"># Set up control function for training</span>

ctrl <span class="o">&lt;-</span> trainControl<span class="p">(</span>method <span class="o">=</span> <span class="s">&quot;repeatedcv&quot;</span><span class="p">,</span>
                     number <span class="o">=</span> <span class="m">10</span><span class="p">,</span>
                     repeats <span class="o">=</span> <span class="m">5</span><span class="p">,</span>
                     summaryFunction <span class="o">=</span> twoClassSummary<span class="p">,</span>
                     classProbs <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>

<span class="c1"># Build a standard classifier using a gradient boosted machine</span>

<span class="kp">set.seed</span><span class="p">(</span><span class="m">5627</span><span class="p">)</span>

orig_fit <span class="o">&lt;-</span> train<span class="p">(</span>Class <span class="o">~</span> <span class="m">.</span><span class="p">,</span>
                  data <span class="o">=</span> imbal_train<span class="p">,</span>
                  method <span class="o">=</span> <span class="s">&quot;gbm&quot;</span><span class="p">,</span>
                  verbose <span class="o">=</span> <span class="kc">FALSE</span><span class="p">,</span>
                  metric <span class="o">=</span> <span class="s">&quot;ROC&quot;</span><span class="p">,</span>
                  trControl <span class="o">=</span> ctrl<span class="p">)</span>


<span class="c1"># Create model weights (they sum to one)</span>

model_weights <span class="o">&lt;-</span> <span class="kp">ifelse</span><span class="p">(</span>imbal_train<span class="o">$</span>Class <span class="o">==</span> <span class="s">&quot;Class1&quot;</span><span class="p">,</span>
                        <span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="kp">table</span><span class="p">(</span>imbal_train<span class="o">$</span>Class<span class="p">)[</span><span class="m">1</span><span class="p">])</span> <span class="o">*</span> <span class="m">0.5</span><span class="p">,</span>
                        <span class="p">(</span><span class="m">1</span><span class="o">/</span><span class="kp">table</span><span class="p">(</span>imbal_train<span class="o">$</span>Class<span class="p">)[</span><span class="m">2</span><span class="p">])</span> <span class="o">*</span> <span class="m">0.5</span><span class="p">)</span>

<span class="c1"># Use the same seed to ensure same cross-validation splits</span>

ctrl<span class="o">$</span>seeds <span class="o">&lt;-</span> orig_fit<span class="o">$</span>control<span class="o">$</span>seeds

weighted_fit <span class="o">&lt;-</span> train<span class="p">(</span>Class <span class="o">~</span> <span class="m">.</span><span class="p">,</span>
                      data <span class="o">=</span> imbal_train<span class="p">,</span>
                      method <span class="o">=</span> <span class="s">&quot;gbm&quot;</span><span class="p">,</span>
                      verbose <span class="o">=</span> <span class="kc">FALSE</span><span class="p">,</span>
                      weights <span class="o">=</span> model_weights<span class="p">,</span>
                      metric <span class="o">=</span> <span class="s">&quot;ROC&quot;</span><span class="p">,</span>
                      trControl <span class="o">=</span> ctrl<span class="p">)</span>

ctrl<span class="o">$</span>sampling <span class="o">&lt;-</span> <span class="s">&quot;down&quot;</span>

down_fit <span class="o">&lt;-</span> train<span class="p">(</span>Class <span class="o">~</span> <span class="m">.</span><span class="p">,</span>
                  data <span class="o">=</span> imbal_train<span class="p">,</span>
                  method <span class="o">=</span> <span class="s">&quot;gbm&quot;</span><span class="p">,</span>
                  verbose <span class="o">=</span> <span class="kc">FALSE</span><span class="p">,</span>
                  metric <span class="o">=</span> <span class="s">&quot;ROC&quot;</span><span class="p">,</span>
                  trControl <span class="o">=</span> ctrl<span class="p">)</span>

ctrl<span class="o">$</span>sampling <span class="o">&lt;-</span> <span class="s">&quot;up&quot;</span>

up_fit <span class="o">&lt;-</span> train<span class="p">(</span>Class <span class="o">~</span> <span class="m">.</span><span class="p">,</span>
                data <span class="o">=</span> imbal_train<span class="p">,</span>
                method <span class="o">=</span> <span class="s">&quot;gbm&quot;</span><span class="p">,</span>
                verbose <span class="o">=</span> <span class="kc">FALSE</span><span class="p">,</span>
                metric <span class="o">=</span> <span class="s">&quot;ROC&quot;</span><span class="p">,</span>
                trControl <span class="o">=</span> ctrl<span class="p">)</span>

ctrl<span class="o">$</span>sampling <span class="o">&lt;-</span> <span class="s">&quot;smote&quot;</span>

smote_fit <span class="o">&lt;-</span> train<span class="p">(</span>Class <span class="o">~</span> <span class="m">.</span><span class="p">,</span>
                   data <span class="o">=</span> imbal_train<span class="p">,</span>
                   method <span class="o">=</span> <span class="s">&quot;gbm&quot;</span><span class="p">,</span>
                   verbose <span class="o">=</span> <span class="kc">FALSE</span><span class="p">,</span>
                   metric <span class="o">=</span> <span class="s">&quot;ROC&quot;</span><span class="p">,</span>
                   trControl <span class="o">=</span> ctrl<span class="p">)</span>

<span class="c1"># Examine results for test set</span>

model_list <span class="o">&lt;-</span> <span class="kt">list</span><span class="p">(</span>original <span class="o">=</span> orig_fit<span class="p">,</span>
                   weighted <span class="o">=</span> weighted_fit<span class="p">,</span>
                   down <span class="o">=</span> down_fit<span class="p">,</span>
                   up <span class="o">=</span> up_fit<span class="p">,</span>
                   SMOTE <span class="o">=</span> smote_fit<span class="p">)</span></code></pre></div>

<h2 id="issues-with-using-roc-for-imbalanced-classes">Issues with using ROC for imbalanced classes</h2>

<p>While using the AUC as an evaluation metric for classifiers on data with imbalanced classes is a popular choice, it can be a misleading one if you are not careful. Take the following example from <a href="http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf">Davis and Goadrich (2006)</a>. Below we see the model performance for two classifiers on an imbalanced dataset, with the ROC curve on the left and the precision-recall curve on the right. In the left example, the AUC for Curve 1 is reported in the paper as 0.813 and the AUC for Curve 2 is 0.875. So blindly choosing the best AUC value will choose Model 2 as the best. However, the precision-recall curve on the right tells a much different story. Here the area under Curve 1 is 0.513 and for Curve 2 it is 0.038. Due to Curve 1 having much better early retrieval compared to Curve 2, we see this massive discrepancy in the precision and recall performance between the two classifiers.</p>

<center><img src="/figs/2017-01-03-imbalanced-classes-part-2/roc_pr_compare.png" /></center>

<p>Another example of how the ROC curve can be misleading comes from <a href="http://people.inf.elte.hu/kiss/11dwhdm/roc.pdf">Fawcett (2005)</a>. Here, we have two datasets, one that has perfect balance between two classes (1:1) and the other has moderate imbalance (10:1). The column on the left shows the ROC curves of two classifiers for both datasets being identical, with the classifier represented by the dashed line having better early retrieval than the classifier represented by the solid line. Again, the precision-recall curve displayed on the right highlights a large discrepancy in performance between the two classifiers on the two datasets. In the case of balanced classes, both classifiers have consistently good precision and recall performance across all thresholds. On the imbalanced data, however, the classifier with better early retrieval has much better precision for lower values of recall.</p>

<center><img src="/figs/2017-01-03-imbalanced-classes-part-2/roc_pr_balance_imbalance.png" /></center>

<p>For these reasons, <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432">Saito and Rehmsmeier (2015)</a> recommend examining the precision-recall curve as it is more explicitly informative than a ROC curve in the case of imbalanced classes. We can calculate the area under the precision-recall curve for our 5 classifiers using the PRROC package in R to create a custom function, <code>calc_auprc</code>. Here, we see a similar story to what was found with the AUC metric in the <a href="http://dpmartin42.github.io/blogposts/r/imbalanced-classes-part-1">last blog post</a>; we have better performance for the weighted model, followed by the sampled models, with the original model coming in last. However, now the difference in performance is much more apparent.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">calc_auprc <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>model<span class="p">,</span> data<span class="p">){</span>
  
  index_class2 <span class="o">&lt;-</span> data<span class="o">$</span>Class <span class="o">==</span> <span class="s">&quot;Class2&quot;</span>
  index_class1 <span class="o">&lt;-</span> data<span class="o">$</span>Class <span class="o">==</span> <span class="s">&quot;Class1&quot;</span>
  
  predictions <span class="o">&lt;-</span> predict<span class="p">(</span>model<span class="p">,</span> data<span class="p">,</span> type <span class="o">=</span> <span class="s">&quot;prob&quot;</span><span class="p">)</span>
  
  pr.curve<span class="p">(</span>predictions<span class="o">$</span>Class2<span class="p">[</span>index_class2<span class="p">],</span>
           predictions<span class="o">$</span>Class2<span class="p">[</span>index_class1<span class="p">],</span> curve <span class="o">=</span> <span class="kc">TRUE</span><span class="p">)</span>
  
<span class="p">}</span>

<span class="c1"># Get results for all 5 models</span>

model_list_pr <span class="o">&lt;-</span> model_list <span class="o">%&gt;%</span>
  map<span class="p">(</span>calc_auprc<span class="p">,</span> data <span class="o">=</span> imbal_test<span class="p">)</span>

model_list_pr <span class="o">%&gt;%</span>
  map<span class="p">(</span><span class="kr">function</span><span class="p">(</span>the_mod<span class="p">)</span> the_mod<span class="o">$</span>auc.integral<span class="p">)</span></code></pre></div>

<div class="highlight"><pre><code class="language-text" data-lang="text">## $original
## [1] 0.5271963
## 
## $weighted
## [1] 0.6463645
## 
## $down
## [1] 0.6293265
## 
## $up
## [1] 0.6459151
## 
## $SMOTE
## [1] 0.6195916</code></pre></div>

<p>We can dig deeper into these results by actually plotting the precision-recall curves. Below, we see that both up sampling and weighting offer the best precision and recall performance depending on the threshold that is chosen, while the original classifier is essentially the worst performing across all thresholds. For example, the weighted classifier simultaneously has a recall of 75% and a precision of 50%, resulting in an F1 score of 0.6, while the original classifier has a recall of 75% and a precision of 25%, resulting in an F1 score of 0.38. In other words, when both classifiers create their predictions and use a particular threshold to obtain hard classifications, they both correctly identify 75% of the cases that are actually in the minority class. However, the weighted classifier is more efficient in these predictions, in that 50% of the observations predicted to be in the minority class actually are, while for the original classifier, only 25% of the observations predicted to be in the minority class actually are.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1"># Plot the AUPRC curve for all 5 models</span>

results_list_pr <span class="o">&lt;-</span> <span class="kt">list</span><span class="p">(</span><span class="kc">NA</span><span class="p">)</span>
num_mod <span class="o">&lt;-</span> <span class="m">1</span>

<span class="kr">for</span><span class="p">(</span>the_pr <span class="kr">in</span> model_list_pr<span class="p">){</span>
  
  results_list_pr<span class="p">[[</span>num_mod<span class="p">]]</span> <span class="o">&lt;-</span> <span class="kt">data_frame</span><span class="p">(</span>recall <span class="o">=</span> the_pr<span class="o">$</span>curve<span class="p">[,</span> <span class="m">1</span><span class="p">],</span>
                                           precision <span class="o">=</span> the_pr<span class="o">$</span>curve<span class="p">[,</span> <span class="m">2</span><span class="p">],</span>
                                           model <span class="o">=</span> <span class="kp">names</span><span class="p">(</span>model_list_pr<span class="p">)[</span>num_mod<span class="p">])</span>
  
  num_mod <span class="o">&lt;-</span> num_mod <span class="o">+</span> <span class="m">1</span>
  
<span class="p">}</span>

results_df_pr <span class="o">&lt;-</span> bind_rows<span class="p">(</span>results_list_pr<span class="p">)</span>

custom_col <span class="o">&lt;-</span> <span class="kt">c</span><span class="p">(</span><span class="s">&quot;#000000&quot;</span><span class="p">,</span> <span class="s">&quot;#009E73&quot;</span><span class="p">,</span> <span class="s">&quot;#0072B2&quot;</span><span class="p">,</span> <span class="s">&quot;#D55E00&quot;</span><span class="p">,</span> <span class="s">&quot;#CC79A7&quot;</span><span class="p">)</span>

ggplot<span class="p">(</span>aes<span class="p">(</span>x <span class="o">=</span> recall<span class="p">,</span> y <span class="o">=</span> precision<span class="p">,</span> group <span class="o">=</span> model<span class="p">),</span> data <span class="o">=</span> results_df_pr<span class="p">)</span> <span class="o">+</span>
  geom_line<span class="p">(</span>aes<span class="p">(</span>color <span class="o">=</span> model<span class="p">),</span> size <span class="o">=</span> <span class="m">1</span><span class="p">)</span> <span class="o">+</span>
  scale_color_manual<span class="p">(</span>values <span class="o">=</span> custom_col<span class="p">)</span> <span class="o">+</span>
  geom_abline<span class="p">(</span>intercept <span class="o">=</span> <span class="kp">sum</span><span class="p">(</span>imbal_test<span class="o">$</span>Class <span class="o">==</span> <span class="s">&quot;Class2&quot;</span><span class="p">)</span><span class="o">/</span><span class="kp">nrow</span><span class="p">(</span>imbal_test<span class="p">),</span>
              slope <span class="o">=</span> <span class="m">0</span><span class="p">,</span> color <span class="o">=</span> <span class="s">&quot;gray&quot;</span><span class="p">,</span> size <span class="o">=</span> <span class="m">1</span><span class="p">)</span> <span class="o">+</span>
  theme_bw<span class="p">()</span></code></pre></div>

<p><img src="/figs/2017-01-03-imbalanced-classes-part-2/unnamed-chunk-3-1.png" alt="center" /></p>

<h2 id="implementing-the-area-under-the-precision-recall-curve-metric-in-caret">Implementing the area under the precision-recall curve metric in caret</h2>

<p>One might imagine wanting to choose hyperparameters of a classifier by using the area under the precision-recall curve rather than the AUC, as some combinations of hyperparameters for a given model might have better early retrieval performance compared to others. It is incredibly easy to create a custom summary function in caret to allow you to do this by combining the code for the <code>calc_auprc</code> function with <a href="https://topepo.github.io/caret/model-training-and-tuning.html#metrics">these instructions</a> found in the caret documentation.</p>

<p>This custom summary function is implemented below, along with code to re-run the original model now using area under the precision-recall curve as the evaluation metric. Here, we see that the original model implementation has the exact same results on the test set for the area under the PR curve, regardless of whether we build the model using the area under the ROC curve or the area under the precision-recall curve. This is because both select the same combination of hyperparameters to build the final model on. Note that this may not always be the case, especially if you decide to use the <a href="http://www.stat.cmu.edu/~ryantibs/datamining/lectures/19-val2-marked.pdf">1-SE</a> rule when choosing the hyperparameters in order to encourage a more parsimonious solution.</p>

<div class="highlight"><pre><code class="language-r" data-lang="r">auprcSummary <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span>data<span class="p">,</span> lev <span class="o">=</span> <span class="kc">NULL</span><span class="p">,</span> model <span class="o">=</span> <span class="kc">NULL</span><span class="p">){</span>
  
  index_class2 <span class="o">&lt;-</span> data<span class="o">$</span>obs <span class="o">==</span> <span class="s">&quot;Class2&quot;</span>
  index_class1 <span class="o">&lt;-</span> data<span class="o">$</span>obs <span class="o">==</span> <span class="s">&quot;Class1&quot;</span>
  
  the_curve <span class="o">&lt;-</span> pr.curve<span class="p">(</span>data<span class="o">$</span>Class2<span class="p">[</span>index_class2<span class="p">],</span>
                        data<span class="o">$</span>Class2<span class="p">[</span>index_class1<span class="p">],</span> curve <span class="o">=</span> <span class="kc">FALSE</span><span class="p">)</span>
  
  out <span class="o">&lt;-</span> the_curve<span class="o">$</span>auc.integral
  <span class="kp">names</span><span class="p">(</span>out<span class="p">)</span> <span class="o">&lt;-</span> <span class="s">&quot;AUPRC&quot;</span>
  
  out
  
<span class="p">}</span>

<span class="c1"># Re-initialize control function to remove smote and</span>
<span class="c1"># include our new summary function</span>

ctrl <span class="o">&lt;-</span> trainControl<span class="p">(</span>method <span class="o">=</span> <span class="s">&quot;repeatedcv&quot;</span><span class="p">,</span>
                     number <span class="o">=</span> <span class="m">10</span><span class="p">,</span>
                     repeats <span class="o">=</span> <span class="m">5</span><span class="p">,</span>
                     summaryFunction <span class="o">=</span> auprcSummary<span class="p">,</span>
                     classProbs <span class="o">=</span> <span class="kc">TRUE</span><span class="p">,</span>
                     seeds <span class="o">=</span> orig_fit<span class="o">$</span>control<span class="o">$</span>seeds<span class="p">)</span>

orig_pr <span class="o">&lt;-</span> train<span class="p">(</span>Class <span class="o">~</span> <span class="m">.</span><span class="p">,</span>
                 data <span class="o">=</span> imbal_train<span class="p">,</span>
                 method <span class="o">=</span> <span class="s">&quot;gbm&quot;</span><span class="p">,</span>
                 verbose <span class="o">=</span> <span class="kc">FALSE</span><span class="p">,</span>
                 metric <span class="o">=</span> <span class="s">&quot;AUPRC&quot;</span><span class="p">,</span>
                 trControl <span class="o">=</span> ctrl<span class="p">)</span>

<span class="c1"># Get results for auprc on the test set</span>

orig_fit_test <span class="o">&lt;-</span> orig_fit <span class="o">%&gt;%</span>
  calc_auprc<span class="p">(</span>data <span class="o">=</span> imbal_test<span class="p">)</span> <span class="o">%&gt;%</span>
  <span class="p">(</span><span class="kr">function</span><span class="p">(</span>the_mod<span class="p">)</span> the_mod<span class="o">$</span>auc.integral<span class="p">)</span>

orig_pr_test <span class="o">&lt;-</span> orig_pr <span class="o">%&gt;%</span>
  calc_auprc<span class="p">(</span>data <span class="o">=</span> imbal_test<span class="p">)</span> <span class="o">%&gt;%</span>
  <span class="p">(</span><span class="kr">function</span><span class="p">(</span>the_mod<span class="p">)</span> the_mod<span class="o">$</span>auc.integral<span class="p">)</span>

<span class="c1"># The test errors are the same</span>

<span class="kp">identical</span><span class="p">(</span>orig_fit_test<span class="p">,</span>
          orig_pr_test<span class="p">)</span></code></pre></div>

<div class="highlight"><pre><code class="language-text" data-lang="text">## [1] TRUE</code></pre></div>

<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1"># Because both chose the same</span>
<span class="c1"># hyperparameter combination</span>

<span class="kp">identical</span><span class="p">(</span>orig_fit<span class="o">$</span>bestTune<span class="p">,</span>
          orig_pr<span class="o">$</span>bestTune<span class="p">)</span></code></pre></div>

<div class="highlight"><pre><code class="language-text" data-lang="text">## [1] TRUE</code></pre></div>

<h2 id="final-thoughts">Final thoughts</h2>

<p>The area under the precision-recall curve can be a useful metric to help differentiate between two competing models in the case of imbalanced classes. For the AUC, weights and sampling techniques may only provide modest improvements. However, this improvement typically impacts early retrieval performance, resulting in a much larger gain in the overall precision of a model. In conjunction with trying weighting or sampling, it is also recommended to avoid relying solely on the AUC when evaluating the performance of a classifier that has imbalanced classes as it can be a misleading metric. The code above shows how easy it is to use the precision-recall curve, a more sensitive measure of classification performance when there are imbalanced classes.</p>

<p><strong>edit (1/3/17):</strong> Graciously pointed out to me by Max Kuhn, a recent release of caret allows you to use the <code>prSummary</code> function rather than a custom function in <code>trainControl</code> to calculate the area under the precision-recall curve. Additionally, the <code>confusionMatrix</code> function has a mode argument that will focus on precision and recall (rather than sensitivity and specificity). See <a href="https://topepo.github.io/caret/measuring-performance.html">this page</a> for more information.</p>

  
  <hr>

  <!-- Bio here -->
  <section class="meta">
    <h3>About</h3>
    <section class="copy">

      <p>
        I am a data scientist for Organizational Solutions at McKinsey & Company, interested in
        statistics, data mining/machine learning, education, and open science, among other things. 
        All opinions and views are my own and do not represent my employer. 
      </p>
      
      
      
      <a href="/blogposts/r/imbalanced-classes-part-2#disqus_thread">Comments</a>

    </section>
    
    <p>


</section>

      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if(!checkbox.checked ||
             sidebar.contains(target) ||
             (target === checkbox || target === toggle)) return;

          checkbox.checked = false;
        }, false);
      })(document);
    </script>
  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53291521-1', 'auto');
  ga('send', 'pageview');

</script>  
    
  </body>
</html>
